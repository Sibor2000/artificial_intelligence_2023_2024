{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Datasets</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.random import rand\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Features</h1>\n",
    "<ul>\n",
    "    <li>Suppose we want to store data about objects, such as houses.</li>\n",
    "    <li><b>Features</b> describe the houses, e.g.\n",
    "        <ul>\n",
    "            <li>$\\mathit{flarea}$: the total floor area (in square metres);</li>\n",
    "            <li>$\\mathit{bdrms}$: the number of bedrooms;</li>\n",
    "            <li> $\\mathit{bthrms}$: the number of bathrooms.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A particular house has <b>values</b> for the features:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\mathit{flarea} = 126, \\mathit{bdrms} = 3, \\mathit{bthrms} = 1$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we can represent a house using a vector:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\cv{126\\\\3\\\\1}$\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will always use $n$ to refer to the number of features, e.g. above $n = 3$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Examples</h1> \n",
    "<ul>\n",
    "    <li>Suppose we collect a <b>dataset</b> containing data about lots of houses, e.g.:\n",
    "        $$\\cv{126\\\\3\\\\1} \\,\\, \\cv{92.9\\\\3\\\\2} \\,\\,\\cv{171.9\\\\4\\\\3} \\,\\, \\cv{79\\\\3\\\\1}$$\n",
    "    </li>\n",
    "    <li>Each member of this dataset is called an <b>example</b>, and we will use $m$ to refer to the number of examples, e.g.\n",
    "        above $m = 4$.\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset notation</h1>\n",
    "<ul>\n",
    "    <li>We will use a <em>superscript</em> to index the examples.\n",
    "        <ul>\n",
    "            <li>\n",
    "                $\\v{x}^{(i)}$ will be the $i$th example.\n",
    "            </li>\n",
    "            <li>\n",
    "                The first example in the dataset is $\\v{x}^{(1)}$, the second is $\\v{x}^{(2)}$, $\\ldots$, \n",
    "                the last is $\\v{x}^{(m)}$ (Note, we index from 1.)\n",
    "            </li>\n",
    "            <li>\n",
    "                We're writing the superscript in parentheses to make it clear that we are using it for indexing.\n",
    "                It is not 'raising to a power'. If we want to raise to a power, we will drop the parentheses.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will use a <em>subscript</em> to index the features (again starting from 1).</li>\n",
    "    <li>Class exercise. Using the dataset from above:\n",
    "        <ul>\n",
    "            <li>what is $\\v{x}_2^{(1)}$?</li>\n",
    "            <li>what is $\\v{x}_1^{(2)}$?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset as a matrix</h1>\n",
    "<ul>\n",
    "    <li>We can represent a dataset $\\Set{\\v{x}^{(1)}, \\v{x}^{(2)}, \\ldots, \\v{x}^{(m)}}$ as a $m \\times n$\n",
    "        matrix $\\v{X}$ as follows:\n",
    "        $$\\v{X} = \\begin{bmatrix}\n",
    "              \\v{x}_1^{(1)} & \\v{x}_2^{(1)} & \\ldots & \\v{x}_n^{(1)} \\\\\n",
    "              \\v{x}_1^{(2)} & \\v{x}_2^{(2)} & \\ldots & \\v{x}_n^{(2)} \\\\\n",
    "              \\vdots        & \\vdots        & \\vdots & \\vdots \\\\\n",
    "              \\v{x}_1^{(m)} & \\v{x}_2^{(m)} & \\ldots & \\v{x}_n^{(m)} \\\\\n",
    "              \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "    <li>Note how each example becomes a <em>row</em> in $\\v{X}$.</li>\n",
    "    <li>You can think of row $i$ as the transpose of $\\v{x}^{(i)}$.</li>\n",
    "    <li>For the example dataset, we get:\n",
    "        $$\\v{X} = \n",
    "            \\begin{bmatrix}\n",
    "                126 & 3 & 1 \\\\\n",
    "                92.9 & 3 & 2 \\\\\n",
    "                171.9 & 4 & 3 \\\\\n",
    "                79 & 3 & 1\n",
    "            \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Cork Property Prices Dataset</h1>\n",
    "<ul>\n",
    "    <li>In August 2019, I scraped a dataset of property prices for Cork city from www.daft.ie.</li>\n",
    "    <li>They are in a CSV file. Each line in the file is an example, representing one house.</li>\n",
    "    <li>Hence, each line of the file contains the feature-values for the floor area, number of bedrooms, number of\n",
    "        bathrooms, and several other features that we will ignore for now.\n",
    "    </li>\n",
    "    <li>We will use the pandas library:\n",
    "        <ul>\n",
    "            <li>to read the dataset from the csv file into what pandas calls a DataFrame;</li>\n",
    "            <li>to explore the dataset: looking at values and computing summary statistics.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we will extract some of the features (columns) and convert to a numpy 2D array, before using the data\n",
    "        to find houses similar to yours.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using pandas to Read and Explore the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flarea', 'bdrms', 'bthrms', 'price'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    float64\n",
       "bdrms       int64\n",
       "bthrms      int64\n",
       "price       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 464 entries, 0 to 463\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   flarea  464 non-null    float64\n",
      " 1   bdrms   464 non-null    int64  \n",
      " 2   bthrms  464 non-null    int64  \n",
      " 3   price   464 non-null    int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 14.6 KB\n"
     ]
    }
   ],
   "source": [
    "# The columns and datatypes (again) but also whether there are any nulls in the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>125.460151</td>\n",
       "      <td>3.329741</td>\n",
       "      <td>2.120690</td>\n",
       "      <td>352.297414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>70.692202</td>\n",
       "      <td>1.068445</td>\n",
       "      <td>1.061033</td>\n",
       "      <td>197.464495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>235.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>575.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           flarea       bdrms      bthrms        price\n",
       "count  464.000000  464.000000  464.000000   464.000000\n",
       "mean   125.460151    3.329741    2.120690   352.297414\n",
       "std     70.692202    1.068445    1.061033   197.464495\n",
       "min     40.000000    1.000000    1.000000    95.000000\n",
       "25%     82.000000    3.000000    1.000000   235.000000\n",
       "50%    110.000000    3.000000    2.000000   295.000000\n",
       "75%    140.600000    4.000000    3.000000   395.000000\n",
       "max    575.000000    9.000000    6.000000  1495.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111.9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flarea  bdrms  bthrms  price\n",
       "0   111.9      3       3    305\n",
       "1    95.0      3       3    255\n",
       "2   120.8      3       3    275"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few of the examples\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Convert to a numpy 2D array</h2>\n",
    "<ul>\n",
    "    <li>We will select certain features (columns) from the pandas DataFrame\n",
    "        and convert to a 2D numpy array\n",
    "    </li>\n",
    "    <li>(Later in the module, we will use a <code>ColumnTransformer</code> to do this.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract these features and convert to numpy 2D array\n",
    "X = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111.9,   3. ,   3. ],\n",
       "       [ 95. ,   3. ,   3. ],\n",
       "       [120.8,   3. ,   3. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X - to show you that we now have a 2D numpy array\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Similarity &amp; Distance</h1>\n",
    "<ul>\n",
    "    <li>In AI, we often want to know how <em>similar</em> one object is to another.\n",
    "        <ul>\n",
    "            <li>E.g. how similar is my house to yours?</li>\n",
    "            <li>E.g. which house in our dataset is most similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In fact, here we are instead going to measure how <em>different</em> they are using a <b>distance function</b>.\n",
    "        <ul>\n",
    "            <li>(N.B. This is not about geographical distance.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let $\\v{x}$ be one vector of feature values and $\\v{x}'$ be another.</li>\n",
    "    <li>Simplest is to measure their <b>Euclidean distance</b>:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{(\\v{x}_1 - \\v{x}_1')^2 + (\\v{x}_2 - \\v{x}_2')^2 + \\ldots + (\\v{x}_n - \\v{x}_n')^2}$$\n",
    "        or, more concisely:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{\\sum_{j=1}^n(\\v{x}_j - \\v{x}_j')^2}$$\n",
    "    </li>\n",
    "    <li>Euclidean distance has a minimum value of 0 (meaning identical) but no maximum value (depends on your data).</li>\n",
    "    <li>Class exercise. What is the Euclidean distance between $\\v{x} = \\cv{100\\\\1\\\\4}$ and $\\v{x}' = \\cv{100\\\\5\\\\1}$?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Euclidean Distance in numpy</h2>\n",
    "<ul>\n",
    "    <li>It has a nice vectorized implementation (no loop!) using numpy:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc(x, xprime):\n",
    "    return np.sqrt(np.sum((x - xprime)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.026297590440446"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "your_house = np.array([126.0, 3, 1])\n",
    "my_house = np.array([107.0, 2, 1])\n",
    "\n",
    "euc(your_house, my_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can compute the distance between your house and all the houses in X.</li>\n",
    "    <li>(We have to write a loop here, because our <code>euc</code> function is not vectorized.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [euc(your_house, x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.241137595009741, 31.064449134018133, 5.571355310873651]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to show you, here are the first 3 distances\n",
    "dists[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0332473082471605"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better, we can, with one line of code, find the most similar house\n",
    "np.min([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better again, we can find which house is the most similar\n",
    "np.argmin([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    125.74\n",
       "bdrms       3.00\n",
       "bthrms      2.00\n",
       "price     398.00\n",
       "Name: 196, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best of all, we can display the most similar house\n",
    "df.iloc[np.argmin([euc(your_house, x) for x in X])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Problems with Euclidean distance</h1>\n",
    "<ul>\n",
    "    <li>There are at least three problems with Euclidean distance (and many other distance measures too):\n",
    "        <ul>\n",
    "            <li>Features with different scales;</li>\n",
    "            <li>Features that are correlated with each other;</li>\n",
    "            <li>The curse of dimensionality.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Scaling Numeric Values</h1>\n",
    "<ul>\n",
    "    <li>Different numeric-valued features often have very different ranges.\n",
    "        <ul>\n",
    "            <li>E.g. the values for floor area are going to range from a few tens to a few hundreds of square metres.</li>\n",
    "            <li>But the number of bedrooms and bathrooms is going to range from 0 to a dozen or so at most.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        When computing the Euclidean distance, features with large ranges will dominate the distance calculations, \n",
    "        thus giving features with small ranges negligible influence.\n",
    "    </li>\n",
    "    <li>\n",
    "        E.g., consider your house $\\v{x} = \\cv{126\\\\3\\\\1}$ and two others, $\\v{y} = \\cv{131\\\\3\\\\1}$ and\n",
    "        $\\v{z} = \\cv{126\\\\7\\\\1}$. \n",
    "        <ul>\n",
    "            <li><em>Intuitively</em>, which house is more similar to yours, $\\v{y}$ or $\\v{z}$?</li>\n",
    "            <li>Now compute the Euclidean distances.</li>\n",
    "            <li>According to these distances, which house is more similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        The solution is to <b>scale</b> (or 'normalize') the values so that they have similar ranges.\n",
    "    </li>\n",
    "    <li>There are several ways to do this. One is <b>min-max scaling</b>, but the one we'll discuss is <b>standardization</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization</h2>\n",
    "<ul>\n",
    "    <!--\n",
    "    <li>In some cases, you don't want feature values to have the same range but to have the same mean\n",
    "        and even the same variance\n",
    "    </li>\n",
    "    -->\n",
    "    <li>\n",
    "        One idea is <b>mean centering</b>, where you subtract the mean value of the feature.\n",
    "        <ul>\n",
    "            <li>If you do this to all values, some of the new values will be positive and some will be negative and \n",
    "                their mean will be approximately zero.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But better still is <b>standardization</b>, in which you subtract the mean and divide by the standard\n",
    "        deviation:\n",
    "        $$\\v{x}_j \\gets \\frac{\\v{x}_j - \\mu_j}{\\sigma_j}$$\n",
    "        where $\\mu_j$ is the mean of the values for feature $j$ and $\\sigma_j$ is their standard deviation\n",
    "    </li>\n",
    "    <li>\n",
    "        If you use this, then the mean will be approximately zero, the standard deviation will be 1.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>scikit-learn provides a class called <code>StandardScaler</code>.\n",
    "    </li>\n",
    "    <li>It uses means and standard deviations that it calculates from your dataset. (Statisticians would say that it should\n",
    "        use the population mean and standard deviation, but these are generally not known.)\n",
    "    </li>\n",
    "    <li>We create the scaler and then run its <code>fit</code> and <code>transform</code> methods.</li>\n",
    "    <li>(Later in the module, when we are using a <code>ColumnTransformer</code>, running these methods\n",
    "        will be done for us.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19202665, -0.30895098,  0.82962481],\n",
       "       [-0.43134924, -0.30895098,  0.82962481],\n",
       "       [-0.06599286, -0.30895098,  0.82962481]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00764486, -0.30895098, -1.05736495])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's scale your house too\n",
    "# Don't try to understand or copy this code - it's a hack that you won't need\n",
    "your_house = np.array([[126.0, 3, 1]])\n",
    "your_house_scaled = scaler.transform(your_house)[0]\n",
    "your_house_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see what effect this has had, let's see which house is most similar to yours\n",
    "np.argmin([euc(your_house_scaled, x) for x in X_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    122.4\n",
       "bdrms       3.0\n",
       "bthrms      1.0\n",
       "price     295.0\n",
       "Name: 328, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argmin([euc(your_house_scaled, x) for x in X_scaled])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Features that are Correlated</h1>\n",
    "<ul>\n",
    "    <li>Let's start with an extreme example. \n",
    "        <ul>\n",
    "            <li>Suppose one feature is the floor area in square metres and\n",
    "                another is the floor area in square feet.\n",
    "                Then it's clear that, even after scaling, when calculating distances, floor area will have greater\n",
    "                influence than other features, such as the number of bedrooms, because it is in the dataset twice.\n",
    "            </li>\n",
    "            <li>Examples are often less stark. For example, floor area and the number of bedrooms are correlated,\n",
    "                and so their contributions to the distance calculations are not independent of each other.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Ideally the features should be independent (at least, linearly independent).</li>\n",
    "    <li>Yet, few people who use distances do anything about this problem!</li>\n",
    "    <li>Solutions (which we're not covering in detail) include feature weighting and projections to\n",
    "        a new feature space whose features are (linearly) independent (e.g. using Principal Component\n",
    "        Analysis).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Curse of Dimensionality</h1>\n",
    "<ul>\n",
    "    <li>In some datasets, examples have thousands or even millions of features.\n",
    "        <ul>\n",
    "            <li>E.g. datasets from astronomy;</li>\n",
    "            <li>E.g. datasets of images and videos;</li>\n",
    "            <li>E.g. datasets of documents where each unique word is a feature.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Is it better or worse to have more features?\n",
    "        <ul>\n",
    "            <li>Storage and processing costs increase.</li>\n",
    "            <li>Apart from efficiency, intuitively, more features is better:\n",
    "                <ul>\n",
    "                    <li>e.g. describing houses more completely.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>But, counter-intuitively, that isn't true in general.\n",
    "                <ul>\n",
    "                    <li>As the number of features grows, algorithms that use distance and density, will find it harder \n",
    "                        to find good solutions unless the number of examples grows at a greater rate.\n",
    "                    </li>\n",
    "                    <li>The problems that arise as the number of features grows have been called <b>the curse of dimensionality</b>.\n",
    "    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Appendix</h1>\n",
    "<h2>Example of the Curse of Dimensionality</h2>\n",
    "<ul>\n",
    "    <li>The code that follows (which you don't need to study):\n",
    "        <ul>\n",
    "            <li>generates a random dataset where $m = 400$ and $n = 2$ and both features have values in $[0, 1)$;\n",
    "            </li>\n",
    "            <li>computes the Euclidean distance between all pairs of examples;</li>\n",
    "            <li>finds $d_{\\mathit{min}}$, the smallest of these distances;</li>\n",
    "            <li>finds $d_{\\mathit{max}}$, the largest of the distances;</li>\n",
    "            <li>computes the ratio $\\frac{d_{\\mathit{max}}}{d_{\\mathit{min}}}$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It then does this all again but with $n = 3, 4, 5,\\ldots,500$.</li>\n",
    "    <li>Then it plots the ratios that it has computed ($y$-axis, but note its scale) against $n$ ($x$-axis).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt0AAAGDCAYAAAD3W6zoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAwTUlEQVR4nO3de5hkdX3n8fd3ehro4dIDQlSay6DgKAZ1zCyaYBLjZQcjEybEVYmXaFgn5lmzmjWTZbJuJK4GdNYYL0Ql0RCjAVkXJyC4YxJuboLGIYMZEWYhKEIPCgo9cmmhZ+a7f5xTQ01NVXf1dJ2uPt3v1/P001Xn+qtTp6o+53d+53ciM5EkSZJUnUX9LoAkSZI03xm6JUmSpIoZuiVJkqSKGbolSZKkihm6JUmSpIoZuiVJkqSKGbo1L0TEJyLiv1ew3OURcXNEPBQR/7nXy++ViHhxRNzT9PyWiHhx/0pUH73cVhFxXUT8x/Lx6yLiK71Ybj9ExMMR8bQeLeu7EfGyXiyrLiLiuHIbDkwyTUbEibNZrrqLiCdHxA3ld/IH+10eaToW97sAWpgi4rvAk4FdwMPA/wHelpkPdzHvm4D/mJkvagzLzLdWU1J+H7g2M59X0fIrkZnPnmqaiFgGfAcYzMydlReqSxFxHnBiZr5+NtbXzbbaz+V+DvjcVNNFxMXAPZn5rirKsb8y85B+rDciEjgpM++o83oy83vAnm0YEdcBn83Mv6hifQvIWuCHwGE5wxuNzNXPnuYva7rVT6vLH/bnASuA9f0tTlvHA7f0uxCSNBsmq5mvYF0REdPNIccD355p4O6FiLDiUtOTmf75N+t/wHeBlzU9/wBwVdPzc4F/Ax4Cvg38ajn8WcBPeKKGfKwcfjHw3qb53wLcATwAXAEcPUlZfoUiWI8B1wHPKodfU67nJ+W6ntFm3uuA9wL/VE5zJfAkihrOHwPfAJY1Tf9h4O5y3E3AzzeNuxr4YNPzS4FPdyjzUPmaHyy3zzqKGpt9ti9wKrC5XOcPgD8ph38PyLLcDwM/Czy9fN0/oqhN+hywtGW5vwf8K7AD+DxwUNP4M4Gby3X9G3B6OXwY+BRwLzBabrOBNq/rdOBxYKIs0zfL4UeX7+MD5fv6lknez4uBPwO+XC7jH4GnAH9abq/bgBUdttV5wGXAZyj2vVuAlZOs6+Xl8nYAHwOupzgLA/Am4P+WjwP4EHBfuW22Aj9NUWs3Ub7mh4ErJ9v/m5cL/M/y9XwHeEXT+COAvwS2l+M3No07o3x/xij22edM8tqS4oxDY5teCFxVlunrwNMnmfcNwF3lfvTf2Hd/vLEsw73ldjugHHdDud5Hyu3xGuBw4EvA/eXr+RJwTMv2uLMs13eA1zWN+03g1nK+TcDxk6znyHLZYxT72VeBRW1e2x8BHy0fD5bL2ND0ufxJ+R4sK9exGHgfe3+XfKxpG78VuL1c74VAdNimi5r2ix9R7KdHlOO+THGmsHn6bwJnlY+fCfxd+bq2Aa9u+bx8nOL75xGavpdn8D33c+WwHeX/n2tZ1vsoPpfjwImTla/NZ7v58/KyybZLOc//Ar5fluUG4Nnl8E6fvT37fetvC/Bi4B7gv5bL/Osp3peDgM+Ww8fKbfHkTp8b/+b/X98L4N/C/GPvH+FjKELIh5vG/weKoLWI4gfxEeCp5bg3UYaZpumbvxhfQhEYnw8cCHwUuKFDOZ5RLvvlFD+gv08R6hoh4DrKENVh/uvK6Z9OESy/Dfy/8sdgMUV4+8um6V9P8WO1GHhn+cV9UDnuKRSh7CXA6yiCxKEd1nsBRSg4AjgW+BadQ/eNwBvKx4cALywfLyt/YBY3zXdiuS0OBI6i+JH605bl/nP53hxBEWjeWo47leKH7eXl+zYCPLMc90Xgk8DBwE+Vy/itDq/tPIrT8M3DbqAI0gdRnBm5H3hJh/kvLt//nymnv4YijL0RGKAID9d22FbnUQSjXy6nPR/4Wof1HEkR9F5V7ju/C+ykfeheRXGQtZQigD+LJ/bni2k6YOxy/5+gOLAcAH6bImBHOf4qioOhw8ty/WI5fAXF/vWCcr7fKF/7gR1eX2vo/lH5Hi+mCFuXdpjvZIoQ8wsU+9GflNulsY1/BnhhuZxlFPvQO9qtt3z+JODXgCXAoRQhamM57mCK0Le8fP5UnghVZ1J8Np9VrutdwD9Nsp7zgU+U22wQ+HnaBGCKz+fW8vHPUYStrzeNaxwoLqPp80Wb75Jy/JfK/eI4iv369A7b9e3A1yi+Lw+k+DxdUo57I/CPLe/BWDndwRQH+m8ut8MKis/HyU3v7Q7gNIr97aA2676OLr/nKL4XHqQ48FoMnF0+f1LTsr4HPLscPzxZ+Tp8vt/bzXYpx/8mxX5zIMWB982dltVhv9gzDUXo3gm8v1ze0BTvy29RHKAsofjM/QxFs5i+/wb715+/vhfAv4X5R/Fj/zBFaEngH2iqUW0z/c3AmeXjNzF56P4U8IGmcYdQhJRlbZb734HLmp4voqiJfXH5/DqmDt3/ren5B4EvNz1f3fwl32b+B4HnNj3/tfIH6IfAiyaZ706afpwpam06he4bKGrnjmxZxjJaQneb9awBtrQs9/VNzz8AfKJ8/EngQ22W8WTgMWCoadjZNAXflunPoyl0UxxU7KLpAIQiIF3cYf6LgT9vev47wK1Nz0+hPEPSZludB/x907iTgfEO63kjTYGcIkzfQ/vQ/RKKkPJCWmpPafPD38X+f0fTuCXl+/gUitC5Gzi8zTI+DvyPlmHbKEN5m+lbQ/dfNI37ZeC2DvP9IU2BnCL0PU6bGtRy/DuAL7Zbb4fpnwc82LTsMYrPzVDLdF8Gzml6vgh4lCdqu1vD1XuAv51s3eV0jdrsJ1HUcP5B+b4fQvE5+0i7zxedQ/eLmp5fBpzbYb23Ai9tev5Uiu+1xRSh8pGm1/Y+yrNkFAdtX21Z1ieBdze9t5+Z4jVfR5ffcxRh+59b5r8ReFPTst7TNG7S8rUpy8XsHbo7bpc28y4tt/lwp89em/1izzQUoftx9j67N9n78ptMcUbJv4X1Z5tu9dOazDyU4ovsmRQ1hwBExBvLXkPGImKM4lT8kW2Xsq+jKU5tA5DFxZk/oqh5nWra3RSht920nfyg6fF4m+fNF1P9XkTcGhE7ytc1zN6v60qKGpFtmfl/J1nn0WU5G+7qNCFwDkWN/m0R8Y2IOKPThGXPAJdGxGhE/Jji1Gjrdv9+0+NHeeL1HUtR69fqeIqaw3ub3s9PUtR4d+No4IHMfKhp2F1M/h51/Z600fr6DurQdnOv9yAzk73fE5rGXUPRjOJC4L6IuCgiDutUgC72/z1lzMxHy4eHULwHD2Tmg20WezzwzsYyy+UeW76ObnR631u1bpdHKD5/jdf2jIj4UkR8v9zH/phJPtsRsSQiPhkRd5XT3wAsjYiBctmvoWiicW9EXBURz2x6vR9ueq0PUBwYddpvNlDU5n4lIu6MiHPbTZSZ4xTNtX6Rojb/eopgdVo57PpOr6WDbrfr8cAXm17PrRQHo08uPxtXAa8tpz2bJy7iPR54Qcv7/jqKg7SGtvtti24/U3t9p5ZaP6/N6+umfJPpuF0iYiAiLoiIfyv3ne+W83T7W9LO/Zn5k27WT9H8ZBNwaURsj4gPRMTgDNatmjN0q+8y83qK2oT/CRARxwN/DryN4pTkUormE9GYZYpFbqf4IqRc3sEUtVKjXUwbFEGk3bQzEhE/T9F85dUUNZFLKU7rRtNk76P40n5qRJw9yeLuLcvZcFynCTPz9sw8myLkvh/4QrlN2m3HPy6Hn5KZh1E0h4k207VzN8Xp53bDH6OoaV9a/h2WnXsNaS3XduCIiDi0adhxVPAeTdNe70HTvtNWZn4kM3+Govb8GRTt8KHl9Xax/0/mbopttbTDuPc1vQdLM3NJZl7SxXKno3W7LKH4/DV8nKId/EnlPvYHTP7a3gksB15QTv8LjUUDZOamzHw5RQ3jbRTbDorX+1str3coM/+p3Uoy86HMfGdmPo3iOo//EhEv7VCm6ynOXqygaKd7PUUTolMpDgrarmKS19iNuyna7je/noMys/E5uAQ4OyJ+lqJZ1bVN813fMt8hmfnbPSxbs72+U0utn9fm9XVTvslMtl1+naKZ0csoKjiWlfNM9lvyKMXZo4bW8N86T8f1Z+ZEZv5RZp5M0RTpDIozZFqgDN2aK/4UeHlEPJfilHFStG8kIt5MUdPX8APgmIg4oMOyLgHeHBHPi4gDKYLk1zPzu22mvQx4ZUS8tKyBeCdFQGz7wzxDh1K0B7wfWBwRfwjsqe2MiF+gaNf4Ror2th+NiE61cpcB6yPi8Ig4hqIJRVsR8fqIOKqsxR8rB+8uy7EbaO6L+VCKZj87ynWvo3ufotjuL42IRRExEhHPzMx7ga8AH4yIw8pxT4+IX+ywnB8Ayxq9GmTm3RTvx/kRcVBEPIei9v6z0yhbFa4Cnh0RZ5U14f+ZDrVzEfHvIuIF5T72CEXzhN3l6B+w93sw1f7fUbmtvwz8WblvDJb7FRRh9K1lOSIiDo6IV7YczPTCF4AzIuJF5Wf0Pez9W3MoRTvsh8ta6dZw1bo9DqWoSR2LiCOAdzdGlGdmziwPIh+j2Hcb2/UTFJ+RZ5fTDkfEf+i0nog4IyJOLA+edlDUVu6mvespPqffzszHKZuOAN/JzPs7zNP6uqbrE8D7yoMyIuKoiDizafzVFGH3PcDny887FG3GnxERbyj3h8Fyf3zWDMoymavL9f16RCyOiNdQHGh+qcP0My3fZNvlUIr94kcUQfqPW+Zt957cDPx6WUt+OsXZi/1af0T8UkScEkWPMD+maHbSaZ/SAmDo1pxQ/lB9BvjDzPw2RZvBGym+FE+huNK94RqKXiW+HxE/bLOsv6doq/2/KWrdns4Tp11bp91GUZv7UYp21KspujJ8vDevbC+bKPoj/38Up1t/QnmaNYqmBp+h6IFgNDO/ShFi/7IMAa3+qFzGdygC7V9Pst7TgVsi4mGK3lNem5njZbOE9wH/WJ4afWG53OdThI6rgMu7fXGZ+c8UBw0fKue/nidqvN4IHEBxAdaDFMHsqR0W9b/K/z+KiH8pH59NUUu1neKizHeX73PfZOYPKS54vIDiR/0k9t5Pmx1GEXof5IlePTaU4z4FnFy+Bxu72P+n8gaKH/fbKC6cfEdZ3s0UF19+rCzHHRTtw3sqM28B/hPwNxSfvwcp2jw3/B5FDeRDFNvk8y2LOA/4q3J7vJrigHyI4vP5NYrPUMMi4L9Q7BcPUASk3y7L8UWKMzuXlk0LvgW8YpL1nAT8PUVwvxH4s8y8lvb+qSxTo1b72xSf50613FB89l4VEQ9GxEcmmW6y+a+gaP7yEMW2eEFjZGY+RvF5fRnFtm8Mfwj49xTfgdspmrM0LgTsucz8EUWN7jsp9vPfB84oPy/tpp9p+SbbLp+h+LyNUrxHX2uZd6/PXjns7RS/A2MUzVw2MrnJ1v8Uiu+6H1Ocwbyeyb+rNc81rnaXJEmSVBFruiVJkqSKGbolSZKkihm6JUmSpIoZuiVJkqSKGbolSZKkirW7y1ptHHnkkbls2bJ+F0OSJEnz3E033fTDzDxqf+evdehetmwZmzdv7ncxJEmSNM9FxF0zmd/mJZIkSVLFDN2SJElSxQzdkiRJUsUM3ZIkSVLFDN2SJElSxQzdkiRJUsVqGbojYnVEXLRjx45+F0WSJEmaUi1Dd2ZemZlrh4eH+10USZIkaUq1DN2SJElSndQ6dN/2/Yc44dyrOO2Ca9i4ZbTfxZEkSZLaqnXonti1mwRGx8ZZf/lWg7ckSZLmpFqH7mbjE7vYsGlbv4shSZIk7WPehG6A7WPj/S6CJEmStI95FbqPXjrU7yJIkiRJ+5g3oXtocIB1q5b3uxiSJEnSPhb3uwAzMTiwiKCo4V63ajlrVoz0u0iSJEnSPmodup/5lEPZfMEr+10MSZIkaVLzpnmJJEmSNFfVMnRHxOqIuGjHjh39LookSZI0pVqG7sy8MjPXDg8P97sokiRJ0pRqGbolSZKkOjF0S5IkSRUzdEuSJEkVM3RLkiRJFTN0S5IkSRUzdEuSJEkVM3RLkiRJFTN0S5IkSRUzdEuSJEkVM3RLkiRJFTN0S5IkSRUzdEuSJEkVM3RLkiRJFTN0S5IkSRUzdEuSJEkVm1OhOyIOjojNEXFGv8siSZIk9UqloTsiPh0R90XEt1qGnx4R2yLijog4t2nUfwUuq7JMkiRJ0myruqb7YuD05gERMQBcCLwCOBk4OyJOjoiXA98G7qu4TJIkSdKsWlzlwjPzhohY1jL4VOCOzLwTICIuBc4EDgEOpgji4xFxdWburrJ8kiRJ0myoNHR3MALc3fT8HuAFmfk2gIh4E/DDToE7ItYCawGOO+64aksqSZIk9cCcupASIDMvzswvTTL+osxcmZkrjzrqqNksmiRJkrRf+hG6R4Fjm54fUw6TJEmS5qV+hO5vACdFxAkRcQDwWuCK6SwgIlZHxEU7duyopICSJElSL1XdZeAlwI3A8oi4JyLOycydwNuATcCtwGWZect0lpuZV2bm2uHh4d4XWpIkSeqxqnsvObvD8KuBq6tctyRJkjRXzLkLKSVJkqT5ppah2zbdkiRJqpNahm7bdEuSJKlOahm6JUmSpDoxdEuSJEkVq2Xotk23JEmS6qSWods23ZIkSaqTSvvpng0bt4yyYdM2to+Nc/TSIdatWs6aFSP9LpYkSZK0R61D99ijE6y/fCvjE7sAGB0bZ/3lWwEM3pIkSZozatm8pNGme/S+H+4J3A3jE7vYsGlbn0omSZIk7auWobvRpnv34JK247ePjc9yiSRJkqTOahm6GwYH2hf/6KVDs1wSSZIkqbNah+6nHHYQQ4MDew0bGhxg3arlfSqRJEmStK9ah+6lSwY5/6xTGFk6RAAjS4c4/6xTvIhSkiRJc0qtey+BopcSQ7YkSZLmslrWdHtHSkmSJNVJLUO3d6SUJElSndQydEuSJEl1YuiWJEmSKmboliRJkipm6JYkSZIqVsvQbe8lkiRJqpNahm57L5EkSVKd1DJ0S5IkSXVi6JYkSZIqZuiWJEmSKmboliRJkipm6JYkSZIqZuiWJEmSKmboliRJkipWy9DtzXEkSZJUJ7UM3d4cR5IkSXVSy9AtSZIk1YmhW5IkSaqYoVuSJEmqmKFbkiRJqpihW5IkSaqYoVuSJEmqmKFbkiRJqpihW5IkSaqYoVuSJEmqmKFbkiRJqlgtQ3dErI6Ii3bs2NHvokiSJElTqmXozswrM3Pt8PBwv4siSZIkTamWoVuSJEmqE0O3JEmSVDFDtyRJklQxQ7ckSZJUMUO3JEmSVDFDtyRJklQxQ7ckSZJUMUO3JEmSVDFDtyRJklQxQ7ckSZJUMUO3JEmSVDFDtyRJklQxQ7ckSZJUMUO3JEmSVDFDtyRJklSxORO6I+JZEfGJiPhCRPx2v8sjSZIk9UqloTsiPh0R90XEt1qGnx4R2yLijog4FyAzb83MtwKvBk6rslySJEnSbKq6pvti4PTmARExAFwIvAI4GTg7Ik4ux/0KcBVwdcXlkiRJkmZNpaE7M28AHmgZfCpwR2bemZmPA5cCZ5bTX5GZrwBeV2W5JEmSpNm0uA/rHAHubnp+D/CCiHgxcBZwIJPUdEfEWmAtwHHHHVdZISVJkqRe6UfobiszrwOu62K6i4CLAFauXJnVlkqSJEmauX70XjIKHNv0/JhymCRJkjQv9SN0fwM4KSJOiIgDgNcCV0xnARGxOiIu2rFjRyUFlCRJknqp6i4DLwFuBJZHxD0RcU5m7gTeBmwCbgUuy8xbprPczLwyM9cODw/3vtCSJElSj1Xapjszz+4w/GrsFlCSJEkLxJy5I6UkSZI0X9UydNumW5IkSXVSy9Btm25JkiTVSS1DtyRJklQnhm5JkiSpYrUM3bbpliRJUp3UMnTbpluSJEl1UsvQLUmSJNWJoVuSJEmqWC1Dt226JUmSVCe1DN226ZYkSVKd1DJ0S5IkSXVi6JYkSZIqZuiWJEmSKmboliRJkipWy9Bt7yWSJEmqk1qGbnsvkSRJUp3UMnRLkiRJdWLoliRJkipm6JYkSZIqZuiWJEmSKlbL0G3vJZIkSaqTWoZuey+RJElSndQydEuSJEl1YuiWJEmSKmboliRJkipm6JYkSZIqZuiWJEmSKmboliRJkipm6JYkSZIqVsvQ7c1xJEmSVCe1DN3eHEeSJEl1UsvQLUmSJNWJoVuSJEmqmKFbkiRJqpihW5IkSaqYoVuSJEmqmKFbkiRJqtjibieMiOcCP18+/WpmfrOaIkmSJEnzS1c13RHxduBzwE+Vf5+NiN+psmCSJEnSfNFtTfc5wAsy8xGAiHg/cCPw0aoKJkmSJM0X3bbpDmBX0/Nd5TBJkiRJU+i2pvsvga9HxBfL52uAT1VSoi5ExGpg9YknntivIkiSJEldi8zsbsKI5wMvKp9+NTO3VFaqLq1cuTI3b97c72JIkiRpnouImzJz5f7OP2lNd0Qclpk/jogjgO+Wf41xR2TmA/u7YkmSJGmhmKp5yd8AZwA3Ac1V4lE+f1pF5ZIkSZLmjUlDd2aeUf4/YXaKI0mSJM0/XV1IGRH/kJkvnWpYv2zcMsqGTdvYPjbO0UuHWLdqOWtWjPS7WJIkSRIwdZvug4AlwJERcThPdBN4GDAnUu3GLaOsv3wr4xNFj4ajY+Osv3wrgMFbkiRJc8JU/XT/FkV77meW/xt/fwt8rNqidWfDpm17AnfD+MQuNmza1qcSSZIkSXubqk33h4EPR8TvZOacvPvk9rHxaQ2XJEmSZltXbboz86MR8dPAycBBTcM/U1XBunX00iFG2wTso5cO9aE0kiRJ0r66ug18RLwb+Gj590vAB4BfqbBcXVu3ajlDgwN7DRsaHGDdquV9KpEkSZK0t65CN/Aq4KXA9zPzzcBzgeHKSjUNa1aMcP5ZpzCydIgARpYOcf5Zp3gRpSRJkuaMrpqXAD/JzN0RsTMiDgPuA46tsFzTsmbFiCFbkiRJc9aUoTsiAvjXiFgK/DlF7yUPAzdWWzRJkiRpfpgydGdmRsSpmTkGfCIi/g9wWGb+a+WlkyRJkuaBbtt0/0tE/DuAzPyugVuSJEnqXrdtul8AvC4i7gIeobgzZWbmcyormSRJkjRPdBu6V1VaCkmSJGke6/bmOHdVXRCAiFgDvBI4DPhUZn5lNtYrSZIkVanbNt37LSI+HRH3RcS3WoafHhHbIuKOiDgXIDM3ZuZbgLcCr6m6bJIkSdJsqDx0AxcDpzcPiIgB4ELgFRS3lj87Ik5umuRd5XhJkiSp9ioP3Zl5A/BAy+BTgTsy887MfBy4FDgzCu8HvpyZ/9JueRGxNiI2R8Tm+++/v9rCS5IkST0wGzXd7YwAdzc9v6cc9jvAy4BXRcRb282YmRdl5srMXHnUUUdVX1JJkiRphrrtvWRWZOZHgI/0uxySJElSL/WrpnsUOLbp+THlsK5ExOqIuGjHjh09L5gkSZLUa/0K3d8AToqIEyLiAOC1wBXdzpyZV2bm2uHh4coKKEmSJPXKbHQZeAlwI7A8Iu6JiHMycyfwNmATcCtwWWbeUnVZJEmSpH6ovE13Zp7dYfjVwNVVr1+SJEnqt341L5kR23RLkiSpTmoZum3TLUmSpDqpZeiWJEmS6sTQLUmSJFWslqHbNt2SJEmqk1qG7nZtujduGeW0C67hhHOv4rQLrmHjlq7vtSNJkiRVak7dBn5/bdwyyvrLtzI+sQuA0bFx1l++FYA1K0b6WTRJkiSpnjXdrTZs2rYncDeMT+xiw6ZtfSqRJEmS9IRahu7WNt3bx8bbTtdpuCRJkjSbahm6W9t0H710qO10nYZLkiRJs6mWobvVulXLGRoc2GvY0OAA61Yt71OJJEmSpCfMiwspGxdLbti0je1j4xy9dIh1q5Z7EaUkSZLmhHkRuqEI3oZsSZIkzUXzonmJJEmSNJfVMnR7R0pJkiTVSS1Dd7s7UkqSJElzVS1DtyRJklQnhm5JkiSpYoZuSZIkqWKGbkmSJKlihm5JkiSpYrUM3XYZKEmSpDqpZei2y0BJkiTVSS1DtyRJklQnhm5JkiSpYoZuSZIkqWKGbkmSJKlihm5JkiSpYoZuSZIkqWK1DN320y1JkqQ6qWXonqyf7o1bRjntgms44dyrOO2Ca9i4ZbQPJZQkSZKesLjfBeiljVtGWX/5VsYndgEwOjbO+su3ArBmxUg/iyZJkqQFrJY13Z1s2LRtT+BuGJ/YxYZN2/pUIkmSJGmehe7tY+PTGi5JkiTNhnkVuo9eOjSt4ZIkSdJsmFehe92q5QwNDuw1bGhwgHWrlvepRJIkSdI8u5CycbHkhk3b2D42ztFLh1i3arkXUUqSJKmv5lXohiJ4G7IlSZI0l8yr5iWSJEnSXGToliRJkipm6JYkSZIqVsvQHRGrI+KiHTt29LsokiRJ0pRqGboz88rMXDs8PNzvokiSJElTmne9lwBs3DJqt4GSJEmaM+Zd6N64ZZT1l29lfGIXAKNj46y/fCuAwVuSJEl9UcvmJZPZsGnbnsDdMD6xiw2btvWpRJIkSVro5l3o3j42Pq3hkiRJUtXmXeg+eulQ2+HDQ4OzXBJJkiSpMO9C97pVyxlcFPsMf+TxnWzcMtqHEkmSJGmhm3ehe82KEQ45aN/rQyd2pe26JUmS1BfzLnQDjD060Xa47bolSZLUD/MydHdq170owiYmkiRJmnXzMnSvW7WcocGBfYbvymT95VsN3pIkSZpV8zJ0r1kxwvlnncJA7HtBpX12S5IkabbNy9ANRfDendl2nG27JUmSNJvmbeiGzm27Ow2XJEmSqjCvQ3e7tt1DgwOsW7W8TyWSJEnSQjSvQ3ejbffSprtRHjQ4r1+yJEmS5qAFkUAf27l7z+MHH52wBxNJkiTNqjkTuiPiaRHxqYj4Qi+Xu2HTNsYndu01zB5MJEmSNJsqDd0R8emIuC8ivtUy/PSI2BYRd0TEuQCZeWdmntPrMnTqqcQeTCRJkjRbqq7pvhg4vXlARAwAFwKvAE4Gzo6Ik6sqgD2YSJIkqd8qDd2ZeQPwQMvgU4E7yprtx4FLgTO7XWZErI2IzRGx+f77759yenswkSRJUr/1o033CHB30/N7gJGIeFJEfAJYERHrO82cmRdl5srMXHnUUUdNubJGDyYjS4cIYGTpEOefdQprVozM9HVIkiRJXVnc7wI0ZOaPgLdWsew1K0YM2ZIkSeqbfoTuUeDYpufHlMO6FhGrgdUnnnhi1/Ns3DLKhk3b2D42ztFLh1i3arlBXJIkSbOiH81LvgGcFBEnRMQBwGuBK6azgMy8MjPXDg8PdzX9xi2jrL98K6Nj4yQwOjZuX92SJEmaNVV3GXgJcCOwPCLuiYhzMnMn8DZgE3ArcFlm3lJlOeyrW5IkSf1UafOSzDy7w/CrgaurXHcz++qWJElSP82ZO1JOR0SsjoiLduzY0dX09tUtSZKkfqpl6J5um+52fXUHRdvu0y64xrbdkiRJqtSc6TKwSo1eSjZs2sZo2aQky3GNiyqbp5MkSZJ6qZY13ftjzYoR1q1aTrQZ50WVkiRJqlItQ/d023Q3bNi0bU8NdysvqpQkSVJVahm6p9umu2GyYO1FlZIkSapKLUP3/uoUrIPiYktJkiSpCgsqdLfrxQRgyQH7DpMkSZJ6ZUGF7jUrRjj/rFNYOjS41/BHHt/lbeElSZJUmVqG7v29kBKK4H3wgfv2lGgPJpIkSapKLUP3/l5I2eBt4SVJkjSbahm6Z6rTBZUJ3qFSkiRJPbcgQ3enCyrhiTtUGrwlSZLUKwsydDcuqBzpUONt+25JkiT1Ui1D90wupGxYs2KEfzz3JW1vCw+275YkSVLv1DJ0z/RCymZLlwy2HT481H64JEmSNF21DN29snHLKA//ZGfbcY88vtN23ZIkSeqJBR26N2zaxsTubDtuYlfarluSJEk9saBD91TttkfHxq3tliRJ0owt6NDdqb/uZr/7+Zt518ats1AaSZIkzVcLOnRP1l93QwKf+9r3rPGWJEnSfqtl6O5Fl4EwdX/dDQm84/M3e7dKSZIk7ZfIbH8hYR2sXLkyN2/e3LPlnXbBNYxO0c57aHCA8886hTUrRnq2XkmSJM1tEXFTZq7c3/lrWdNdlXWrlne8WU7D+MQuzrvillkpjyRJkuaHxf0uwFyyZsUIm+96gM9+7XuTTjc2PsGyc68C4PAlg7x79bOt+ZYkSVJH1nS3eO+aUzi8w10q23nw0QnWfeGbtvWWJElSR9Z0t/Hu1c9m/eVbGZ/Y1dX0E7uSd172TTbf9QDX3nY/28fGOXrpEOtWLbcGXJIkSYbudhpBecOmbVNeWNmwK3OvZimjY+Osv3zrXsuTJEnSwmTvJVPYuGV0WrXerSKAxJpvSZKkGptp7yW1DN0RsRpYfeKJJ77l9ttvr3x9G7eMct4VtzA2PjGj5QwuCgYHgkcndgNehClJklQXCzJ0N8xGTXezXoXvZoMDwYZXPdfgLUmSNIfNNHTbpnsa1qwY2ROON24Z5Xc/fzMzPWSZ2JVs2LTN0C1JkjSP2WXgflqzYoTXvfC4nixrdGycZedexYr3fMWuByVJkuYha7pn4L1rTmHl8Uf0rMlJo89vaN/jycYto2zYtM0uCSVJkmrGNt09tuI9X+HBR3vX5nsyQ4MDnH/WKQCGcUmSpArZpnuOme6NdWZifGIX511xC4/t3L1nfZP1D25NuSRJUn9Y012BRrjt9sY6VRiI4IOvfi7wxE1+yi7D97JkcBEHDg4w9uiEQVySJKkDuwycg6G72bs2buVzX/vejHs52R+DiwKi6CGlW40mKwZvSZKkJxi653john2bdfzSM4/i2tvu72tN+GQGItidac23JElSydBdg9A9mZneZr5qg4uCQw5azNijEwwPDRKBTVEkSdKC44WUNdcIrb2+02WvTOzOPb2xNJdvsgs2JUmStDdruueQTr2LzIULM7vVfGHm8NAgj+/cxaMTuwE4fMkg7179bEO6JEmqnQXZvCQiVgOrTzzxxLfcfvvt/S7OrOrnhZm9sChgeGiwbShfMljcIHU6Id1uECVJ0mxYkKG7Yb7VdHerU9A87YJralEb3itLBhcxsTs79s5y+JJBXvmcp3LtbfcbyiVJ0owYuhdg6O5ksosyByLYldm2r+6FaqqadGvRJUlSg6Hb0L2XqYJic/twA3hvtTaPmU5zmeb3rdFLzIOPTuw5WBrZj9DvQYMkSb1j6DZ077fmAN4Id9aIz13NNy7q1Pd7I7Q/8tgEZdbfRxVt5dtNDxj6JUnzhqHb0F2ZTkFqrnZvqO4tCjho8aI9tfDT0dpWfnhokEce37lX2/pFQOuSp3u303YHhftT4y9JUi8Yug3dc0a7JhKNXkp+/JMJdtd3V9MsWjK4iIlduzvW1E81L7DXwcSigN3JnsAOk9fAV1lr380ZBJsFSdLcZOg2dNfCxi2je9WQN/fn3do8ovH8S9+81xp11Vq7g4BmAfzc04/glu0Pdb2vNzcPav1cNcZ36rWndfpuevip+iCgnwcZHuBImg5Dt6F7wenUS8vBBwzw6OO72jZ3kDS7GgfWzRcEt14rMlW3n+0OzpsPxtudxejU/K1xgDHZwfzgouCQgxbvOUPXfLau8bibSoF2103MpeskPNiQ9o+h29C9IHXbS0u7Cw27rUlv/KB7camkhazdGZtOZ0mAtmc1Jzv4aj54anxXN1/LsXSSuxu3O3vT7kLxTs0f2/0+THYQ0s2do6u6BqVXB0u9PuiaKwdxs1EOQ7ehW3NAL0J+849TLwP+ksFFjE/s9oBBkuaZAwaCxYuiYxO25oOjqW6e183ZoOlqPlvVehdq2Ldyq/UAq5dlnawibaqmgI2DuV99/jGGbmm+aa2VmeqLqvV/aw3LZDdOaqd1+ZIkLXT3fOKc+3eOff+n9nf+xb0sjKTeWLNipKenxRrLmsnpz8n6dZ8s/M/0otgqal8kSZqugaFDj5rJ/NZ0S5oTZtIeb7J5W9t8TtVWfyanILsxuIj96g5RktRf9/7VO3js3ttjf+c3dEtSj3V7oW/rWYd2F5GNLO3c5R9MflFZ60FC6wVrzWcPenFAIUnzmaHb0C1Jlev2TMRkvURMdgaitdeJyZozdeoisLULwcl6lWh3gNN8MVcEe/W4MVkPGq2vfXRsfFpnUNqp+oyLpOkzdBu6JUmalpncHXWqMyqt3QlOdvDVWE+nm6d100d6aw8dkx3QtOvCsPX/VAdEkx30VH0NSi8PuGZy99+F6t6L3777se/fMbC/88+Z0B0RBwN/BjwOXJeZn5tqHkO3JElaiLppxjadPsz35yL7qa6nmWq53Z4Zm6qsUzXN61RWmPyGU63zfO09v/qdXY/ueNr+vF9QceiOiE8DZwD3ZeZPNw0/HfgwMAD8RWZeEBFvAMYy88qI+Hxmvmaq5Ru6JUmSNBtmenOcRb0sTBsXA6c3D4iIAeBC4BXAycDZEXEycAxwdzlZd50JS5IkSTVQaejOzBuAB1oGnwrckZl3ZubjwKXAmcA9FMF70nJFxNqI2BwRm++///4qii1JkiT1VNU13e2M8ESNNhRhewS4HPi1iPg4cGWnmTPzosxcmZkrjzpqRn2US5IkSbNiztyRMjMfAd7c73JIkiRJvdaPmu5R4Nim58eUwyRJkqR5qR+h+xvASRFxQkQcALwWuGI6C4iI1RFx0Y4dOyopoCRJktRLlYbuiLgEuBFYHhH3RMQ5mbkTeBuwCbgVuCwzb5nOcjPzysxcOzw83PtCS5IkST1WaZvuzDy7w/CrgaurXLckSZI0V/SjecmM2bxEkiRJdVLL0G3zEkmSJNVJpbeBr1pEPARs63c5NOccCfyw34XQnON+oXbcL9SO+4XaWZ6Zh+7vzHOmn+79tC0zV/a7EJpbImKz+4VauV+oHfcLteN+oXYiYvNM5q9l8xJJkiSpTgzdkiRJUsXqHrov6ncBNCe5X6gd9wu1436hdtwv1M6M9otaX0gpSZIk1UHda7olSZKkOa+2oTsiTo+IbRFxR0Sc2+/yaPZExKcj4r6I+FbTsCMi4u8i4vby/+Hl8IiIj5T7yb9GxPP7V3JVJSKOjYhrI+LbEXFLRLy9HO5+sYBFxEER8c8R8c1yv/ijcvgJEfH18v3/fEQcUA4/sHx+Rzl+WV9fgCoVEQMRsSUivlQ+d79Y4CLiuxGxNSJubvRU0svfkVqG7ogYAC4EXgGcDJwdESf3t1SaRRcDp7cMOxf4h8w8CfiH8jkU+8hJ5d9a4OOzVEbNrp3AOzPzZOCFwH8qvxPcLxa2x4CXZOZzgecBp0fEC4H3Ax/KzBOBB4FzyunPAR4sh3+onE7z19uBW5ueu18I4Jcy83lNXUb27HeklqEbOBW4IzPvzMzHgUuBM/tcJs2SzLwBeKBl8JnAX5WP/wpY0zT8M1n4GrA0Ip46KwXVrMnMezPzX8rHD1H8kI7gfrGgle/vw+XTwfIvgZcAXyiHt+4Xjf3lC8BLIyJmp7SaTRFxDPBK4C/K54H7hdrr2e9IXUP3CHB30/N7ymFauJ6cmfeWj78PPLl87L6ywJSnflcAX8f9YsErmxDcDNwH/B3wb8BYZu4sJ2l+7/fsF+X4HcCTZrXAmi1/Cvw+sLt8/iTcL1QclH8lIm6KiLXlsJ79jtT9jpTSPjIzI8JueRagiDgE+N/AOzLzx82VUe4XC1Nm7gKeFxFLgS8Cz+xvidRvEXEGcF9m3hQRL+5zcTS3vCgzRyPip4C/i4jbmkfO9HekrjXdo8CxTc+PKYdp4fpB47RO+f++crj7ygIREYMUgftzmXl5Odj9QgBk5hhwLfCzFKeBG5VOze/9nv2iHD8M/Gh2S6pZcBrwKxHxXYrmqS8BPoz7xYKXmaPl//soDtJPpYe/I3UN3d8ATiqvND4AeC1wRZ/LpP66AviN8vFvAH/bNPyN5VXGLwR2NJ0m0jxRtq/8FHBrZv5J0yj3iwUsIo4qa7iJiCHg5RTt/a8FXlVO1rpfNPaXVwHXpDezmHcyc31mHpOZyyjywzWZ+TrcLxa0iDg4Ig5tPAb+PfAtevg7Utub40TEL1O0yRoAPp2Z7+tviTRbIuIS4MXAkcAPgHcDG4HLgOOAu4BXZ+YDZRj7GEVvJ48Cb87MzX0otioUES8Cvgps5Yk2mn9A0a7b/WKBiojnUFz4NEBRyXRZZr4nIp5GUcN5BLAFeH1mPhYRBwF/TXFNwAPAazPzzv6UXrOhbF7ye5l5hvvFwla+/18sny4G/iYz3xcRT6JHvyO1Dd2SJElSXdS1eYkkSZJUG4ZuSZIkqWKGbkmSJKlihm5JkiSpYoZuSZIkqWKGbkmSJKlihm5JkiSpYoZuSVogImJZRNwaEX8eEbdExFfKOzVKkipm6JakheUk4MLMfDYwBvxaf4sjSQuDoVuSFpbvZObN5eObgGX9K4okLRyGbklaWB5rerwLWNyvgkjSQmLoliRJkipm6JYkSZIqFpnZ7zJIkiRJ85o13ZIkSVLFDN2SJElSxQzdkiRJUsUM3ZIkSVLFDN2SJElSxQzdkiRJUsUM3ZIkSVLFDN2SJElSxf4/4aW4lVyYMKQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 400\n",
    "n_range = range(1, 500)\n",
    "\n",
    "ratios = []\n",
    "for n in n_range:\n",
    "    X = rand(m, n)\n",
    "    dists = euclidean_distances(X)\n",
    "    non_zero_dists = dists[dists > 0]\n",
    "    ratios += [np.max(non_zero_dists) / (np.min(non_zero_dists))]\n",
    "    \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Ratio of max distance to min distance in datasets with ever more features\")\n",
    "plt.scatter(n_range, ratios)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"n\")\n",
    "plt.xlim(0, 500)\n",
    "plt.ylabel(\"ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>As $n \\rightarrow \\infty$, $d_{\\mathit{max}} \\rightarrow d_{\\mathit{min}}$, so their ratio tends to 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2833880745431754,\n",
       " 1.277583326565987,\n",
       " 1.2686440282295635,\n",
       " 1.2847541391800033,\n",
       " 1.2459976099204813]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it may not be clear from the graph, we'll show the last 5 of the ratios that it calculated\n",
    "ratios[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We conclude (counter-intutively) that examples become equi-distant!</li>\n",
    "    <li>This obviously undermines methods that depend on finding objects that are similar to each other, as we were\n",
    "        doing earlier &mdash; with more features, the most similar object becomes more arbitrary!\n",
    "    </li>\n",
    "    <li>The problem extends to other distance/similarity measures, e.g. cosine similarity.</li>\n",
    "    <li>Fortunately, there are lots of methods available for reducing dimensionality.\n",
    "        One solution is to retain the principle components found by Principal Component Analysis. This is\n",
    "        interesting because PCA was suggested above as a solution to the problem of correlated features. \n",
    "        It can actually help us solve both problems.\n",
    "    </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
