{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Overfitting with Neural Networks</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "# Load MNIST into four Numpy arrays\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "mnist_x_train = mnist_x_train.reshape((60000, 28 * 28))\n",
    "mnist_x_test = mnist_x_test.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgement</h1>\n",
    "<ul>\n",
    "    <li>The analogy between dropout and a company whose employees are told to toss a coin to\n",
    "        decide whether to go to work each morning comes from\n",
    "        A. G&eacute;ron: <i>Hands-On Machine Learning with Scikit-Learn, Keras &amp;\n",
    "        TensorFlow (2nd edn)</i>, O'Reilly, 2019\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>One of the central problems of deep learning is overfitting.</li>\n",
    "    <li>Reminder. If your model overfits, your main options are:\n",
    "        <ul>\n",
    "            <li>gather more training examples;</li>\n",
    "            <li>remove noise in the training examples;</li>\n",
    "            <li>change model: move to a less complex model;</li>\n",
    "            <li>simplify by reducing the number of features;</li>\n",
    "            <li>stick with your existing model but\n",
    "                add constraints (if you can) to reduce its complexity.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Here we'll look at\n",
    "        <ul>\n",
    "            <li>reducing the network's size &mdash; an example of moving to a less complex model;</li>\n",
    "            <li>weight regularization &mdash; an example of adding constraints to reduce complexity;</li>\n",
    "            <li>dropout &mdash; also an example of adding constraints to reduce complexity; and</li>\n",
    "            <li>early stopping &mdash; a somewhat different way of avoiding overfitting.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 11:50:14.558126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# A network that overfits (a little!)\n",
    "\n",
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "outputs = Dense(10, activation=\"softmax\")(x)\n",
    "overfitting_model = Model(inputs, outputs)\n",
    "overfitting_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0010371966054663062, 0.13483312726020813)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = overfitting_model.fit(mnist_x_train, mnist_y_train, epochs=20, batch_size=32, \n",
    "                                verbose=0, validation_split=0.20)\n",
    "history.history[\"loss\"][-1], history.history[\"val_loss\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training curve\n",
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reducing Network Size</h1>\n",
    "<ul>\n",
    "    <li>We can make the model (neural network) less complex by reducing the number of parameters.</li>\n",
    "    <li>Obviously enough, this is achieved by:\n",
    "        <ul>\n",
    "            <li>reducing the number of hidden layers, and/or</li>\n",
    "            <li>reducing the number of neurons within the hidden layers.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example of reducing network size</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller network\n",
    "\n",
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "outputs = Dense(10, activation=\"softmax\")(x)\n",
    "smaller_model = Model(inputs, outputs)\n",
    "smaller_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05821022391319275, 0.09794046729803085)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = smaller_model.fit(mnist_x_train, mnist_y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2)\n",
    "history.history[\"loss\"][-1], history.history[\"val_loss\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Weight Regularization</h1>\n",
    "<ul>\n",
    "    <li>For linear regression, we used <b>regularization</b> to ensure that the coefficients $\\v{\\beta}$ took only\n",
    "        small values by penalizing large values in the loss function.\n",
    "        <ul>\n",
    "            <li>Lasso: we penalized by the $\\cal{l}_1$-norm (the sum of their absolute values).\n",
    "            </li>\n",
    "            <li>Ridge: we penalized by the $\\cal{l}_2$-norm (the sum of their squares).\n",
    "            </li>\n",
    "            <li>A hyperparameter $\\lambda$, called the 'regularization parameter' controlled the balance \n",
    "                between fitting the data versus shrinking the parameters.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Weight Regularization in neural networks is the same idea, but applied to the weights \n",
    "        in the layers of a network.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized network\n",
    "\n",
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "outputs = Dense(10, activation=\"softmax\", kernel_regularizer=l2(0.0001))(x)\n",
    "regularized_model = Model(inputs, outputs)\n",
    "regularized_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0574953518807888, 0.11248209327459335)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = regularized_model.fit(mnist_x_train, mnist_y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2)\n",
    "history.history[\"loss\"][-1], history.history[\"val_loss\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Weight regularization can work well when the network is small but has little effect on larger networks.</li>\n",
    "    <li>For larger networks, a better option can be <i>dropout</i>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dropout</h1>\n",
    "<ul>\n",
    "    <li>Imagine we have a layer that uses <b>dropout</b> with <b>droput rate</b> $p$, e.g. $p=0.5$.</li>\n",
    "    <li>Then, in a given step of the backprop algorithm, each neuron in the layer has\n",
    "        probability $p$ of being ignored &mdash; treated as if it were not there.\n",
    "        <figure>\n",
    "            <img src=\"images/dropout.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One way of doing dropout</h2>\n",
    "<ul>\n",
    "    <li>Suppose the droppout rate is $p$.\n",
    "    <li>Training. For any given mini-batch:\n",
    "        <ul>\n",
    "            <li>In the forward propagation, \n",
    "                <ul>\n",
    "                    <li>decide which neurons will be dropped (chosen with probability $p$);</li>\n",
    "                    <li>set the activations of the dropped neurons to zero;</li>\n",
    "                    <li>multiply the activations of the kept neurons by $1 / (1 - p)$.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>In the backpropagation, ignore the dropped out neurons.</li>\n",
    "        </ul>\n",
    "        Note that different neurons will get dropped for each mini-batch.\n",
    "    </li>\n",
    "    <li>Testing. No change.</li>\n",
    "    <li>But why did we multiply activations by $1/ (1 - p)$?\n",
    "        <ul>\n",
    "            <li>In testing, for $p=0.5$ a neuron in the next layer will receive input from on average\n",
    "                twice as many neurons as it did in training.\n",
    "            </li>\n",
    "            <li>The multiplication by $1/(1 - p)$ compensates for this.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why does dropout reduce overfitting?</h2>\n",
    "<ul>\n",
    "    <li>Consider a company whose employees were told to toss a coin every monring to decide\n",
    "        whether to go to work or not.\n",
    "        <ul>\n",
    "            <li>The organization would need to become more resilient. It could not rely on any\n",
    "                one employee to perform critical tasks: the expertise would need to be spread\n",
    "                across many employees. They must become more like generalists, less like specialists.\n",
    "            </li>\n",
    "        </ul>\n",
    "        Similarly, in dropout layers, neurons learn more robust features.     \n",
    "    </li>\n",
    "    <li>Another way to think about it.\n",
    "        <ul>\n",
    "            <li>Since a neuron can be present or absent, it's like training on a different neural\n",
    "                network at each step.\n",
    "            </li>\n",
    "            <li>The final result is  a bit like an ensemble of these many different virtual\n",
    "                neural networks.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>However, it typically increases the number of epochs needed for convergence\n",
    "        (roughly double when $p=0.5$).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dropout in Keras</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with dropout\n",
    "\n",
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(10, activation=\"softmax\")(x)\n",
    "model_with_dropout = Model(inputs, outputs)\n",
    "model_with_dropout.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05792544409632683, 0.11650843173265457)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model_with_dropout.fit(mnist_x_train, mnist_y_train, epochs=20, batch_size=32, verbose=0, validation_split=0.2)\n",
    "history.history[\"loss\"][-1], history.history[\"val_loss\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Early Stopping</h1>\n",
    "<ul>\n",
    "    <li>We know that a sign of overfitting is that validation error stops getting lower and starts\n",
    "        getting larger.\n",
    "    </li>\n",
    "    <li>We can exploit this <em>during</em> Gradient Descent as another way of avoiding overfitting,\n",
    "        known as <b>early stopping</b>:\n",
    "        <ul>\n",
    "            <li>During Gradient Descent, monitor validation error (or loss).</li>\n",
    "            <li>Interrupt training when the validation error has stopped improving for a certain number\n",
    "                of epochs.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Early stopping in Keras</h2>\n",
    "<ul>\n",
    "    <li>In Keras, this is done using the <code>EarlyStopping</code> callback.</li>\n",
    "    <li>The <code>patience</code> argument allows you to specify how many epochs must pass with\n",
    "        no improvement relative to the current best.\n",
    "    </li>\n",
    "    <li><code>restore_best_weights=True</code> restores the weights and biases from when validation\n",
    "        error was at its lowest.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = overfitting_model.fit(mnist_x_train, mnist_y_train, epochs=20, batch_size=32, \n",
    "                                verbose=0, validation_split=0.2, \n",
    "                                callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001325890887528658, 0.14725150167942047),\n",
       " (0.0008877089712768793, 0.15284860134124756),\n",
       " (0.0005975296953693032, 0.1530618965625763)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(l, v) for l, v in zip(history.history[\"loss\"], history.history[\"val_loss\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>An advantage of early stopping is that we can be less concerned about choosing the number of\n",
    "        epochs: just use something very large.\n",
    "    </li>\n",
    "    <li>But, now we have the problem of deciding on the patience. If run-time is your problem, then\n",
    "        you can choose a low value. Otherwise, you should choose a low value for 'easier' problems!\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<h1>Conclusions</h1>\n",
    "<ul>\n",
    "    <li>Overfitting is a major problem but has many solutions.</li>\n",
    "    <li>There are lots of solutions in additon to the ones above:\n",
    "        <ul>\n",
    "            <li>Remember Batch Normalization has a regularizing effect.</li>\n",
    "            <li>There are other techniques that we won't cover (e.g. clipping).</li>\n",
    "            <li>There are the things we've mentioned in an earlier lecture, especially getting more data!</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Appendix</h1>\n",
    "<h2>Over-parameterisation</h2>\n",
    "<ul>\n",
    "    <li>Our discussion so far of overfitting in neural networks follows the conventional viewpoint: on a small network, validation error is high (underfitting); as we increase complexity (e.g. more layers), then validatiion error decreases but, after a certain point, it starts to rise again (overfitting).</li>\n",
    "    <li>In fact, with deep learning, we are finding something counter-intuitive. If we keep increasing the complexity, then validation error decreases again, falling to a new minimum. This is called <b>double descent</b>. (See <a href=\"https://mlu-explain.github.io/double-descent/\">this visualisation</a>, if you're interested.)</li>\n",
    "    <li>We are finding this happens with over-parameterized models &mdash; ones that are excessively complex. Neural networks with many neurons are the obvios examples of this. But we can even have linear models that are overparameterized.\n",
    "    <li>When the model is over-parameterised, it fits the training data (near) perfectly (even the noise). We would expect an overparameterized model to have poor performance on unseen data (overfitting).  But, we find that the model seems to generalise well to unseen data. What's going on?</li>\n",
    "    <li>We don't really know. But the current theory, in high-level terms, is that the over-parameterised model makes smoother interpolations between the training examples, and this is what gives the better validation error. (See <a href=\"https://mlu-explain.github.io/double-descent2/\">this explanation</a>, if you're interested.)</li>\n",
    "    <li>It is also fairly easy to avoid double descent: using the regularization or dropout techniques described in the next lecture.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
