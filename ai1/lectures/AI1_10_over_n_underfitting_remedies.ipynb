{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Underfitting and Overfitting Remedies</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>You are building a predictor but its performance is not good enough.\n",
    "        What should you do?\n",
    "    </li>\n",
    "    <li>Some of the options include:\n",
    "        <ul>\n",
    "            <li>gather more training examples;</li>\n",
    "            <li>remove noise in the training examples;</li>\n",
    "            <li>add more features or remove features;</li>\n",
    "            <li>change model: move to a more complex model or maybe to a less complex model;</li>\n",
    "            <li>stick with your existing model but add constraints to it to reduce its complexity or\n",
    "                remove constraints to increase its complexity.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Surprisingly, \n",
    "        <ul>\n",
    "            <li>gathering more training examples may not help;</li>\n",
    "            <li>adding more features may in some cases worsen the performance;</li>\n",
    "            <li>changing to a more complex model may in some cases worsen the performance</li>\n",
    "        </ul>\n",
    "        &hellip;it all depends on what is causing the poor performance (underfitting or overfitting).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Underfitting</h1>\n",
    "<ul>\n",
    "    <li>If your model underfits:\n",
    "        <ul>\n",
    "            <li>gathering more training examples will not help.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Your main options are:\n",
    "        <ul>\n",
    "            <li>change model: move to a more complex model;</li>\n",
    "            <li>collect data for additional features that you hope will be more predictive;</li>\n",
    "            <li>create new features which you hope will be predictive (see <b>feature engineering</b>\n",
    "                in the next lecture);</li>\n",
    "            <li>stick with your existing model but remove constraints (if you can) to increase \n",
    "                its complexity.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>What additional features do you think would be predictive of Cork property prices?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Overfitting</h1>\n",
    "<ul>\n",
    "    <li>If your model overfits, your main options are:\n",
    "        <ul>\n",
    "            <li>gather more training examples;</li>\n",
    "            <li>remove noise in the training examples;</li>\n",
    "            <li>change model: move to a less complex model;</li>\n",
    "            <li>simplify by reducing the number of features;</li>\n",
    "            <li>stick with your existing model but\n",
    "                add constraints (if you can) to reduce its complexity.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regularization</h1>\n",
    "<ul>\n",
    "    <li>If your model underfits, we saw that one option is:\n",
    "        <ul>\n",
    "            <li>stick with your existing model but\n",
    "                <em>remove</em> constraints (if you can) to increase its complexity.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If your model overfits, we saw that one option is:\n",
    "        <ul>\n",
    "            <li>stick with your existing model but\n",
    "                <em>add</em> constraints (if you can) to reduce its complexity.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Constraining a model to make it less complex and reduce the risk of overfitting is called\n",
    "        <b>regularization</b>. Regularization is a general concept but we will explain it in the \n",
    "        case of linear regression in the rest of this lecture.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regularization for Linear Regression</h1>\n",
    "<ul>\n",
    "    <li>Linear models are among the least complex models.\n",
    "        <ul>\n",
    "            <li>Hence, we normally associate them with underfitting.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But, even linear regression might overfit the training data.</li>\n",
    "    <li>If you are overfitting, you must reduce the degrees of freedom.\n",
    "        <ul>\n",
    "            <li>One way is to discard some features.\n",
    "                <ul>\n",
    "                    <li>Then you have fewer coefficients ($\\v{\\beta}$) that you can modify.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Another way is to constrain the range of values that the coefficients can take:\n",
    "                <ul>\n",
    "                    <li>E.g. force the learning algorithm to only choose small values (close to zero).\n",
    "                        This makes the distrbution of the values of the coefficients more regular.\n",
    "                    </li>\n",
    "                    <li>Recall that OLS linear regression finds coeffficients $\\v{\\beta}$ that minimize\n",
    "                        $$J(\\v{X}, \\v{y}, h_{\\v{\\beta}}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{i)}) - \\v{y}^{(i)})^2$$\n",
    "                        Regularization imposes a penalty on the size of the coefficients.\n",
    "                    </li>\n",
    "                </ul>\n",
    "                This is how we regularize linear regression.\n",
    "                <ul>\n",
    "                    <li>In effect, it <em>penalizes</em> hypotheses that fit the data too well.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lasso Regression: Using the $\\cal{l}_1$-norm</h2>\n",
    "<ul>\n",
    "    <li><b>Lasso Regression</b>:\n",
    "        <ul>\n",
    "            <li>'Lasso' stands for 'least absolute shrinkage and selection operator' &mdash; but\n",
    "                this doesn't matter!\n",
    "            </li>\n",
    "            <li>We penalize by the $\\cal{l}_1$-norm of $\\v{\\beta}$, which is simply the sum of their \n",
    "                absolute values, i.e. \n",
    "                $$\\sum_{j=1}^n|\\v{\\beta}_j|$$\n",
    "            </li>\n",
    "            <li>(Minor point: we don't penalize $\\v{\\beta}_0$, which is why $j$ starts at 1.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So Lasso Regression finds the $\\v{\\beta}$ that minimizes\n",
    "        $$J(\\v{X}, \\v{y}, h_{\\v{\\beta}}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{i)}) - \\v{y}^{(i)})^2\n",
    "        + \\lambda\\sum_{j=1}^n|\\v{\\beta}_j|$$\n",
    "    </li>\n",
    "    <li>$\\lambda$ is called the 'regularization parameter'.\n",
    "        <ul>\n",
    "            <li>It controls how much penalization we want and this determines the balance between the \n",
    "                two parts of the modified loss function: fitting the data versus shrinking the parameters.\n",
    "            </li>\n",
    "            <li>As $\\lambda \\rightarrow 0$, Lasso Regression gets closer to being OLS Linear Regression.</li>\n",
    "            <li>When $\\lambda = 0$, Lasso Regression is the same as OLS Linear Regression.</li>\n",
    "            <li>When $\\lambda \\rightarrow \\infty$, penalties are so great that all the coefficients will tend \n",
    "                to zero: the only way to minimize the loss function will be to make the coefficients\n",
    "                as small as possible. It's likely that in this case we will underfit the data.\n",
    "            </li>\n",
    "        </ul>\n",
    "        So, for regularization to work well, we must choose the value of $\\lambda$ carefully.\n",
    "        <ul>\n",
    "            <li>So what kind of thing is $\\lambda$?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>An important observation about Lasso Regression:\n",
    "        <ul>\n",
    "            <li>As $\\lambda$ grows, some of the $\\v{\\beta}_j$ will be driven to zero.</li>\n",
    "            <li>This means that the model that it learns treats some features as irrelevant.</li>\n",
    "            <li>Hence, it performs some <b>feature selection</b> too.</li>\n",
    "            <li>Compare this with Ridge Regression below.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing Lasso Regression</h3>\n",
    "    <!--\n",
    "<ul>\n",
    "    <li>\n",
    "        There is no equivalent to the Normal Equation.\n",
    "    </li>\n",
    "    <li>\n",
    "        Even Gradient Descent has a problem:\n",
    "        <ul>\n",
    "            <li>The Lasso loss function is not differentiable at $\\v{\\beta}_i = 0$.</li>\n",
    "            <li>scikit-learn uses an approach called 'coordinate descent' (details unimportant).\n",
    "                <ul>\n",
    "                    <li>There is a special class, <code>Lasso</code>.\n",
    "                    </li>\n",
    "                    <li>Or you can use <code>SGDRegressor</code> with <code>penalty=\"l1\"</code>.</li>\n",
    "                </ul>\n",
    "                They both refer to $\\lambda$ as <code>alpha</code>!\n",
    "            </li>\n",
    "            <li>Scaling is usually advised.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "-->\n",
    "<ul>\n",
    "        <li>scikit-learn has a class for this, called <code>Lasso</code>.\n",
    "        </li>\n",
    "        <li>Or you can use <code>SGDRegressor</code> with <code>penalty=\"l1\"</code>.</li>\n",
    "        <li>They both refer to $\\lambda$ as <code>alpha</code>!</li>\n",
    "        <li>Scaling of feature values is usually advised.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ridge Regression: Using the $\\cal{l}_2$-norm</h2>\n",
    "<ul>\n",
    "    <li><b>Ridge Regression</b>:\n",
    "        <ul>\n",
    "            <li>We penalize by the $\\cal{l}_2$-norm, which is simply the sum of the squares of the\n",
    "                coefficients, i.e. \n",
    "                $$\\sum_{j=1}^n\\v{\\beta}_j^2$$ \n",
    "                (Strictly speaking, the $\\cal{l}_2$-norm is the square root of the sum of squares.) \n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So Ridge Regression finds the $\\v{\\beta}$ that minimizes\n",
    "        $$J(\\v{X}, \\v{y}, h_{\\v{\\beta}}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{i)}) - \\v{y}^{(i)})^2\n",
    "        + \\lambda\\sum_{j=1}^n\\v{\\beta}_j^2$$\n",
    "    </li>\n",
    "    <li>Both Lasso and Ridge Regression shrink the values of the coefficients.\n",
    "        <ul>\n",
    "            <li>But, as we mentioned, Lasso Regression may additionally result in coefficients being set to zero.\n",
    "            </li>\n",
    "            <li>This does not happen with Ridge Regression.</li>\n",
    "            <li>Optionally, consult section 3.4.3 of <i>The Elements of Statistical Learning</i> \n",
    "                by Hastie, Friedman &amp; Tibshirani (available online) for an explanation.\n",
    "            </li>\n",
    "            <li>One observation from the book is that, roughly speaking, Lasso Regression shrinks the\n",
    "                coefficients by approximately the same <em>constant amount</em> (unless they are so small \n",
    "                that they get shrunk to zero), whereas, again roughly speaking, Ridge Regression shrinks the\n",
    "                coefficients by approximately the same <em>proportion</em>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing Ridge Regression</h3>\n",
    "<!--\n",
    "<ul>\n",
    "    <li>There is an equivalent to the Normal Equation (solved, e.g., by Cholesky decomposition).\n",
    "        <ul>\n",
    "            <li>Take the gradient, set it equal to zero, and solve for $\\v{\\beta}$ (details unimportant):\n",
    "                $$\\v{\\beta} = (\\v{X}^T\\v{X} + \\lambda\\v{I})^{-1}\\v{X}^T\\v{y}$$\n",
    "            </li>\n",
    "            <li>In the above, $\\v{I}$ is the $(n+1)$ identity matrix, i.e. all zeros except for the main \n",
    "                diagonal which \n",
    "                is all ones. (In fact, for consistency with what we were doing above, where we chose not to penalize\n",
    "                $\\v{\\beta}_0$, you want a zero in the top left, so this is not really the identity matrix.)\n",
    "            </li>\n",
    "            <li>Also, you don't need to implement this with the pseudo-inverse. It's possible to prove that,\n",
    "                provided $\\lambda > 0$, then $\\v{X}^TX + \\lambda\\v{I}$ will be invertible.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Alternatively, use Gradient Descent:\n",
    "        <ul>\n",
    "            <li>The update rule for $\\v{\\beta}_j$ for all $j$ except $j = 0$ becomes:\n",
    "                $$\\v{\\beta}_j \\gets \\v{\\beta}_j - \n",
    "                    \\alpha(\\frac{1}{m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)} +\n",
    "                    \\frac{\\lambda}{m}\\v{\\beta}_j)$$\n",
    "            </li>\n",
    "            <li>We can re-arrange this to:\n",
    "                $$\\v{\\beta}_j \\gets \\v{\\beta}_j(1 - \\alpha\\frac{\\lambda}{m}) - \n",
    "                    \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$$\n",
    "                which helps to show why this shrinks $\\v{\\beta}_j$\n",
    "            </li>\n",
    "            <li>In scikit-learn, there is a special class, <code>Ridge</code> \n",
    "                <ul>\n",
    "                    <li>You can set its <code>solver</code> parameter to choose different methods, or leave it as\n",
    "                        default <code>auto</code>.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Or you can use <code>SGDRegressor</code> with <code>penalty=\"l2\"</code>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Scaling is usually advised</li>\n",
    "</ul>\n",
    "-->\n",
    "<ul>\n",
    "    <li>In scikit-learn, there is a special class for this, called <code>Ridge</code>. \n",
    "                <ul>\n",
    "                    <li>You can set its <code>solver</code> parameter to choose different methods, or leave it as\n",
    "                        default <code>auto</code>.\n",
    "                    </li>\n",
    "                </ul>\n",
    "    </li>\n",
    "    <li>Or you can use <code>SGDRegressor</code> with <code>penalty=\"l2\"</code>.</li>\n",
    "    <li>Scaling of feature values is usually advised.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Illustrating the Effects of Lasso and Ridge Regression</h1>\n",
    "<ul>\n",
    "    <li>We'll generate a random, non-linear dataset.</li>\n",
    "    <li>Then we'll fit an unregularized linear model and two regularized models (Lasso and Ridge).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(m, func, error):\n",
    "    X = np.random.random(m)\n",
    "    y = func(X, error)\n",
    "    return X.reshape(m, 1), y\n",
    "\n",
    "def f(x, error = 1.0):\n",
    "    y = 10 - 1 / (x + 0.1)\n",
    "    if error > 0:\n",
    "        y = np.random.normal(y, error)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_dataset(50, f, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'random_state' parameter of train_test_split must be an int in the range [0, 4294967295], an instance of 'numpy.random.mtrand.RandomState' or None. Got <module 'numpy.random' from 'E:\\\\info\\\\III-1\\\\AI\\\\artificial_intelligence_2023_2024\\\\venv\\\\Lib\\\\site-packages\\\\numpy\\\\random\\\\__init__.py'> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\info\\III-1\\AI\\artificial_intelligence_2023_2024\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:201\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    199\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 201\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n",
      "File \u001b[1;32mE:\\info\\III-1\\AI\\artificial_intelligence_2023_2024\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'random_state' parameter of train_test_split must be an int in the range [0, 4294967295], an instance of 'numpy.random.mtrand.RandomState' or None. Got <module 'numpy.random' from 'E:\\\\info\\\\III-1\\\\AI\\\\artificial_intelligence_2023_2024\\\\venv\\\\Lib\\\\site-packages\\\\numpy\\\\random\\\\__init__.py'> instead."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = np.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to scale here because there is only one feature\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "y_predicted_ols = ols.predict(X_test)\n",
    "mse_ols = mean_squared_error(y_predicted_ols, y_test)\n",
    "\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_predicted_lasso = lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_predicted_lasso, y_test)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_predicted_ridge = ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_predicted_ridge, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the three subplots\n",
    "fig = plt.figure(figsize=(14, 4.5)) \n",
    "gs = gridspec.GridSpec(1, 3) \n",
    "# Leftmost diagram: OLS\n",
    "ax0 = plt.subplot(gs[0])\n",
    "plt.title(\"OLS Linear Regression\\nMSE: %.3f\\nIntercept: %.3f\\nCoefficient: %.3f\" % \n",
    "          (mse_ols, ols.intercept_, ols.coef_[0]))\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.ylim(-4, 14)\n",
    "ax0.scatter(X_train, y_train, color = \"green\")\n",
    "ax0.plot(X_test, y_predicted_ols, color = \"blue\")\n",
    "# Middle diagram: Lasso\n",
    "ax1 = plt.subplot(gs[1])\n",
    "plt.title(\"Lasso Regression\\nMSE: %.3f\\nIntercept: %.3f\\nCoefficient: %.3f\" % \n",
    "          (mse_lasso, lasso.intercept_, lasso.coef_[0]))\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.ylim(-4, 14)\n",
    "ax1.scatter(X_train, y_train, color = \"green\")\n",
    "ax1.plot(X_test, y_predicted_lasso, color = \"blue\")\n",
    "# Righmost diagram: Ridge\n",
    "ax2 = plt.subplot(gs[2])\n",
    "plt.title(\"Ridge Regression\\nMSE: %.3f\\nIntercept: %.3f\\nCoefficient: %.3f\" % \n",
    "          (mse_ridge, ridge.intercept_, ridge.coef_[0]))\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.ylim(-4, 14)\n",
    "ax2.scatter(X_train, y_train, color = \"green\")\n",
    "ax2.plot(X_test, y_predicted_ridge, color = \"blue\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Here is an interactive version so that we can play with the \n",
    "        regularization hyperparameter ($\\lambda$, but called <code>alpha</code>) to see how it\n",
    "        affects the fit.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, alpha):\n",
    "    plt.figure()\n",
    "    plt.title(\"%s with lambda (alpha) = %.1f\" % (model, alpha))\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.ylim(-4, 14)\n",
    "    plt.scatter(X_train, y_train, color = \"green\")\n",
    "    if model == \"lasso\":\n",
    "        model = Lasso(alpha)\n",
    "    else:\n",
    "        model = Ridge(alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    plt.plot(X_test, y_predicted, color = \"blue\")\n",
    "    plt.show()\n",
    "    \n",
    "interactive_plot = interactive(plot_model, {'manual': True}, scale=True, alpha=(0,3,.1), model=[\"lasso\", \"ridge\"]) \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Regularization is a response to overfitting. The problem with the example above is that we\n",
    "        are regularizing a model (linear regression) on a dataset that it underfits!\n",
    "    </li>\n",
    "    <li>To see the value of regularization, let's regularize a model that does overfit. \n",
    "        Let's regularize Polynomial Regression with degree 30.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interactive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot([x \u001b[38;5;28;01mfor\u001b[39;00m x, _ \u001b[38;5;129;01min\u001b[39;00m test_sorted], [y_predicted \u001b[38;5;28;01mfor\u001b[39;00m _, y_predicted \u001b[38;5;129;01min\u001b[39;00m test_sorted], color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 20\u001b[0m interactive_plot \u001b[38;5;241m=\u001b[39m \u001b[43minteractive\u001b[49m(plot_model, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanual\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m.1\u001b[39m), model\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \n\u001b[0;32m     21\u001b[0m interactive_plot\n",
      "\u001b[1;31mNameError\u001b[0m: name 'interactive' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_model(model, alpha):\n",
    "    plt.figure()\n",
    "    plt.title(\"%s with lambda (alpha) = %.1f\" % (model, alpha))\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.ylim(-4, 14)\n",
    "    plt.scatter(X_train, y_train, color = \"green\")\n",
    "    if model == \"lasso\":\n",
    "        model = Pipeline([(\"poly\", PolynomialFeatures(degree=30, include_bias=False)),\n",
    "                          (\"predictor\", Lasso(alpha))])\n",
    "    else:\n",
    "        model = Pipeline([(\"poly\", PolynomialFeatures(degree=30, include_bias=False)),\n",
    "                          (\"predictor\", Ridge(alpha))])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    test_sorted = sorted(zip(X_test, y_predicted))\n",
    "    plt.plot([x for x, _ in test_sorted], [y_predicted for _, y_predicted in test_sorted], color = \"blue\")\n",
    "    plt.show()\n",
    "    \n",
    "interactive_plot = interactive(plot_model, {'manual': True}, scale=True, alpha=(0,3,.1), model=[\"lasso\", \"ridge\"]) \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Concluding Remarks</h1>\n",
    "<ul>\n",
    "    <li>For completeness, we mention Elastic Net, which combines Lasso and Ridge regularization, with yet\n",
    "        another hyperparameter to control the balance between the two.\n",
    "    </li>\n",
    "    <li>Using some regularization is usually better than none, and Ridge is a good default.\n",
    "    </li>\n",
    "    <li>But we now have an extra hyperparameter whose value we must choose.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
